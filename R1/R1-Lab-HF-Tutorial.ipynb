{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定 APIKEY\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Intorduction to 🤗 Hugging Face and Transformers Library**\n",
    "\n",
    "[🤗 hugging face NLP course](https://huggingface.co/learn/nlp-course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NLP 是語言學和機器學習領域，專注於理解與人類語言相關的一切。 NLP 任務的目標不僅是單獨理解單字，而且能夠理解這些單字的上下文。</p>\n",
    "NLP 常見的任務有：\n",
    "   1. **Classifying whole sentences 文本分類（分類整句）**：將整個句子進行分類\n",
    "      - 情感分析：「這部電影很棒！」 -> positive\n",
    "      - 垃圾郵件檢測：「你的 iphone 已被嚴重損壞」-> spam \n",
    "   2. **Classifying each word in a sentence 單詞分類**：對一句話中的所有字進行分類\n",
    "      - 語法分析：「他跑得快」-> 他（代詞）跑（動詞）得（副詞）快（形容詞）\n",
    "      - 命名實體 NER\n",
    "   3. **Sentence Generation**\n",
    "      1. **填充遮蔽詞**：「像這種要求，我這輩子[mask]！」-> [mask] 預測為 沒聽過\n",
    "      2. **自動生成**：「今天天氣如何」-> 「今天天氣非常晴朗適合外出」\n",
    "      3. **翻譯**：「你好」-> 「Hello」\n",
    "      4. **摘要（問答）**\n",
    "         1. 從文本中提取答案 Extractive QA：「法國的首都是巴黎。   首都在哪」-> 巴黎（藉由巴黎在原文的 index 抓出來）\n",
    "         2. 以生成模型進行摘要 Generative QA：「法國的首都是巴黎。   首都在哪」-> 首都在巴黎（使用生成模型，例如 chatgpt）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`pipeline` in Hugging Face transformers**\n",
    "``transformers`` 為 🤗Hugging face 提供的套件，讓開發者可以創建、使用 Hugging face hub 上 NLP、LLM 的模型</p>\n",
    "> Hugging face hub 上的模型不只有 transformer，任何人都可以上傳任何類型的模型或資料集\n",
    "\n",
    "在 `transformers` 中最高階的函數是 `pipeline`，\n",
    "該函數將使用模型需要的預處理、推理與後處理串連起來，</p>\n",
    "傳入指定的 task，`pipeline` 會自動以適合的模型進行推理（預測）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以從 [hub](https://huggingface.co/models) 透過 Tasks、Languages 篩選找到自己想要應用的模型</p>\n",
    "從 1. [task summary](https://huggingface.co/docs/transformers/task_summary) 2. [Tasks](https://huggingface.co/tasks) 找到支援的 NLP 相關任務"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **使用 pipeline 完成常見 NLP 任務**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 情感分析 aka 分類問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998723268508911}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"sentiment-analysis\")\n",
    "pipe(\"this is awesome!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 命名實體</p>\n",
    "\n",
    "\n",
    "\n",
    "在 ner 任務中，模型會對所有字詞（token） 進行分類，得到：</p>\n",
    "1. 該字詞（token）對應的 entity\n",
    "2. score 機率值\n",
    "3. 以及對應到文本的起始結束位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(task=\"ner\", \n",
    "                # model='dslim/bert-base-NER'\n",
    "                )\n",
    "ner_pipe(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **練習 #1**\n",
    "\n",
    "使用 Extractive QA model 以 `pipeline` 做 question-answering 任務：\n",
    "- **給定文本**：the name of repo is bert-base-uncased\n",
    "- **問題目標**：問模型 repo 的名稱\n",
    "- **預期答案**：bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# practice 1 不需要特別指定模型，pipeline 預設載入 distilbert-base-cased-distilled-squad, \n",
    "# 其為 Extractive QA 類摘要模型\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **利用 Conversation class 與 text-generation model 實作 chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為載入模型較大，使用 T4 GPU 時建議進行量化，以下程式為量化處理過程，</p>\n",
    "在此先不贅述，有興趣的可以參考 Hugging face 官方文件～</p>\n",
    "\n",
    "與前面範例不同的是，模型載入方法，我們透過 `AuToModelForCausalLM` 實例化模型，將其作為參數傳入 `pipeline`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'MediaTek-Research/Breeze-7B-32k-Instruct-v1_0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=hf_model, \n",
    "    tokenizer=tokenizer, # Tokenizer，要與模型匹配，主要提供 chat 模式時的特殊符號\n",
    "    max_new_tokens=1024, # 模型最多可以生成多少字\n",
    "    return_full_text=False # 控制 pipeline 只輸出 AI Message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聊天式模型，例如 ChatGPT 其實基本上是透過 text-generation 作為基礎模型，進一步訓練模型能過聊天。\n",
    "所以模型的選擇，我們可以在 Huggingface 上找到 text-generation 任務的模型，應該都可以支援。</p>\n",
    "\n",
    "比較特別的是，要做聊天任務時，模型需要一些特殊符號來區別每一段訊息是來自於 User 或是 AI 還是 System Prompt</p>\n",
    "而各個模型的特殊符號不盡相同，需要去查閱官方文件。例如 Demo 使用的聯發科 Breeze 模型是透過 `[INST]` 、 `[/INST]` 以及 `<s>` 作為區隔。\n",
    "所以我們在使用模型時，就會需要將文字加上這些特殊符號才能夠發揮模型聊天的能力。</p>\n",
    "\n",
    "通常我們會使用 list of dict 的方式處存聊天的記錄，使用 role 區別 user 與 ai，content 代表內容，而 Hugging face 的模型也支援這樣的格式，例如：\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"嗨你好嗎\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"嗨您好，我是您的 AI 助理，很高興為您服務。\"},\n",
    "    {\"role\": \"user\", \"content\": \"掰掰\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"掰掰，期待再相見\"},\n",
    "\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "我們可以透過實例化 Conversation 這個 class，透過 `add_user_input` 與 `append_response` 新增歷史使用者輸入與模型回覆，將資料變為上述的資料格式再送給模型進行推理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "conversation = Conversation() # 建立一個對話 Conversation 物件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 `add_user_input` 新增 user 聊天記錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前聊天記錄：[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}]\n"
     ]
    }
   ],
   "source": [
    "conversation.add_user_input(\"provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?\")\n",
    "print(f\"目前聊天記錄：{conversation.messages}\") # conversation.messages 可以直接丟給 chatbot 得到回覆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '根據提供的信息，repo的名称是\"bert-base-uncased\"。'}]\n"
     ]
    }
   ],
   "source": [
    "# 將 conversation.messages 丟給 chatbot\n",
    "chatbot_result = chatbot(conversation.messages)\n",
    "print(chatbot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 chatbot 的回覆以 `append_respons` 的方法加入 conversation 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前聊天記錄：[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}, {'role': 'assistant', 'content': '根據提供的信息，repo的名称是\"bert-base-uncased\"。'}]\n"
     ]
    }
   ],
   "source": [
    "conversation.append_response(chatbot_result[0]['generated_text'])\n",
    "print(f\"目前聊天記錄：{conversation.messages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前聊天記錄：[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}, {'role': 'assistant', 'content': '根據提供的信息，repo的名称是\"bert-base-uncased\"。'}, {'role': 'user', 'content': '那什麼是 bert?'}]\n"
     ]
    }
   ],
   "source": [
    "conversation.add_user_input(\"那什麼是 bert?\")\n",
    "\n",
    "print(f\"目前聊天記錄：{conversation.messages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 回覆：[{'generated_text': ' Bert是Bidirectional Embedding Representations from Transformers的缩写，是Google的一个自然语言处理模型。它基于Transformers架构，可以对文本数据学习语义表达，用于各种自然语言处理任务，如情感分析、命名实体识别、情况语言模型等。Bert模型通过预训练在大量的非任务专用数据上获得知识，然后在特定的任务上进行微调。'}]\n",
      "----------\n",
      "目前聊天記錄：[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}, {'role': 'assistant', 'content': '根據提供的信息，repo的名称是\"bert-base-uncased\"。'}, {'role': 'user', 'content': '那什麼是 bert?'}, {'role': 'assistant', 'content': ' Bert是Bidirectional Embedding Representations from Transformers的缩写，是Google的一个自然语言处理模型。它基于Transformers架构，可以对文本数据学习语义表达，用于各种自然语言处理任务，如情感分析、命名实体识别、情况语言模型等。Bert模型通过预训练在大量的非任务专用数据上获得知识，然后在特定的任务上进行微调。'}]\n"
     ]
    }
   ],
   "source": [
    "chatbot_result = chatbot(conversation.messages)\n",
    "print(f\"LLM 回覆：{chatbot_result}\")\n",
    "\n",
    "print(\"-\"*10)\n",
    "conversation.append_response(chatbot_result[0]['generated_text'])\n",
    "print(f\"目前聊天記錄：{conversation.messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **embedding model (feature extraction)**\n",
    "[參考](https://huggingface.co/tasks/feature-extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding 是將文字轉換成向量的技術，使得文字可以在數學空間中表示。</p>這些向量捕捉了文字之間的語義關係，使得相似的文字在向量空間中更接近。常見的嵌入模型包括 Word2Vec、GloVe 和 BERT 等。\n",
    "\n",
    "在 Retrieval-Augmented Generation (RAG) 中，我們會用 Embedding model 用來將查詢（query）和候選文檔（document）轉換成向量。</p>\n",
    "通過計算這些向量的相似度，可以找出與查詢最相關的文檔。這些相關文檔隨後用來生成回答，增強生成模型的準確性和上下文相關性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們使用 sentence transformers 這個套件，可以從 [官方文檔](https://sbert.net/docs/pretrained_models.html) 尋找自己希望使用的 model，也可以在 hugging face 平台上搜尋支援 feature-extraction 的模型。</p>\n",
    "\n",
    "另外 hugging face 也提供 [embedding model](https://huggingface.co/spaces/mteb/leaderboard) 的排行榜給大家參考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/verdict/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03142083, -0.01894771, -0.00766944, ..., -0.02039895,\n",
       "       -0.01210877,  0.03742645], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用 encode 得到 sentence 的 embedding\n",
    "embedding_model.encode(\"哈囉，這是一個句子\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"為什麼 ML 需要做正規化\"\n",
    "\n",
    "source_sentence = [\n",
    "    'Regularization is important!',\n",
    "    'Dropout is important!',\n",
    "    'Missing Data Handling is important!'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們有每個句子的 embedding 後就可以透過 cosine similarity 計算每個文本的相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "為什麼 ML 需要做正規化 vs Regularization is important! similarity: 0.8665976524353027\n",
      "為什麼 ML 需要做正規化 vs Dropout is important! similarity: 0.8055095672607422\n",
      "為什麼 ML 需要做正規化 vs Missing Data Handling is important! similarity: 0.8144524693489075\n",
      "==========\n",
      "與「為什麼 ML 需要做正規化」最相似文本：Regularization is important!\n"
     ]
    }
   ],
   "source": [
    "most_related_sentence = None\n",
    "max_similarity = 0\n",
    "\n",
    "for sentence in source_sentence:\n",
    "    sim = calculate_cosine_similarity(\n",
    "    embedding_model.encode(query),\n",
    "    embedding_model.encode(sentence)\n",
    "    )\n",
    "    \n",
    "    if sim > max_similarity:\n",
    "        most_related_sentence = sentence\n",
    "        max_similarity = sim\n",
    "        \n",
    "    print(f\"{query} vs {sentence} similarity: {sim}\")\n",
    "    \n",
    "print(\"=\"*10)\n",
    "print(f\"與「{query}」最相似文本：{most_related_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作業 demo**\n",
    "\n",
    "利用 Hugging Face 的 text-generation model 與 Sentence Transformers embedding model 實作 QA 檢索聊天機器人。</p>\n",
    "基於提供的資料集，使用 Embedding Cosine Similarity 檢索參考資料，再透過 LLM 生成答案。</p>\n",
    "\n",
    "1. Baseline\n",
    "   - 將 demo 中的資料，替換成我們提供 or 自己的資料集\n",
    "   - 能夠檢索相似資料\n",
    "   - 基於檢索的資料進行回答\n",
    "2. Advanced（Optional）\n",
    "   - Embedding 怎麼儲存？每次都要重新計算嗎？\n",
    "   - 該如何處理太久以前的歷史資料？\n",
    "   - 利用 Gradio or Hugging Face Spaces 部署、分享 Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = [\n",
    "    \"\"\"1. 入營注意事項：役男入營前無須先行理髮，待入營後統一安排理髮，再向役男收取費用\n",
    "    2. 役男可攜帶手機(不得為大陸廠牌，且不提供充電)，部隊會集中保管，於每日夜間以定時定點方式使用。\n",
    "    3. 營區不提供充電，建議攜帶拋棄式(手動)刮鬍刀；不可攜帶噴霧式液體。\n",
    "    4. 役男可配戴隱形眼鏡，但因基礎訓練期間生活緊湊，而隱形眼鏡需相當時間消毒清洗，建議配戴鏡片眼鏡為宜，並可多備一副眼鏡，以供替換。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"報到入營時應該文件：\n",
    "    1. 徵集令。\n",
    "    2. 役男本人之國民身分證正本。\n",
    "    3. 私章。\n",
    "    4. 健保ＩＣ卡。\n",
    "    5. 郵局存摺正面影本。\n",
    "    6. 替代役役男輔導需求調查表。\n",
    "    7. 個人特殊醫療用品。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    替代役訓練天數\n",
    "    基礎訓練：21天(含撥交日)。(自253梯次起修正)\n",
    "    專業訓練：以各分發需用機關不同而有個不同期間的專業訓練。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    折抵役期規定\n",
    "    請備妥相關證明文件，如高中（職）以上各級學校經軍訓主管驗證加註折抵役期日數之成績單、大專集訓結訓證書、\n",
    "    驗退（停役）證明書或軍事學校退學（開除）證明書等正本（驗證後退還）。82年次以前出生者，\n",
    "    合計不得逾30日，83-93年次以後出生者，合計不得逾15日。已受軍事入伍訓練者，請於收到徵集令時，向戶籍地區公所提出免受基礎訓練申請。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    要當兵時，健保要辦理轉出轉入嗎?\n",
    "    即將入營服常備兵役之役男需持徵集令向全民健保加保單位辦理轉出，轉出日期填報「入營當月份」，轉入單位免填，後續將由國軍單位接續辦理後續轉入程序。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"當兵期間義務役薪水的入帳戶一定要郵局嗎?\n",
    "    依現行規定目前有郵局、台新、土地銀行跟合作金庫這四家金融機構可使用。\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def get_answer(query: str, source: List[str]):\n",
    "    most_related_sentence = None\n",
    "    max_similarity = 0\n",
    "\n",
    "    for sentence in source:\n",
    "        sim = calculate_cosine_similarity(\n",
    "        embedding_model.encode(query),\n",
    "        embedding_model.encode(sentence)\n",
    "        )\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            most_related_sentence = sentence\n",
    "            max_similarity = sim\n",
    "            \n",
    "    return most_related_sentence\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 我明天要去軍營報到，我應該要帶什麼文件\n",
      "AI: 明天你要去軍營報到，你應該要帶以下文件：\n",
      "\n",
      "1. 徵集令\n",
      "2. 役男本人之國民身分證正本\n",
      "3. 私章\n",
      "4. 健保IC卡\n",
      "5. 郵局存摺正面影本\n",
      "6. 替代役役男輔導需求調查表\n",
      "7. 個人特殊醫療用品\n",
      "\n",
      "以上文件請務必備妥並確認後，明天前往軍營進行報到。\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\">>>\")\n",
    "conversation = Conversation()\n",
    "\n",
    "while user_query.lower() != \"bye\":\n",
    "    print(f\"user: {user_query}\")\n",
    "    # 尋找最相似的文件\n",
    "    answer = get_answer(user_query, qa_data)\n",
    "    llm_input = f\"\"\"請你基於以下資訊回答使用者的問題\n",
    "    {answer}\n",
    "    ===\n",
    "    問題：{user_query}\n",
    "    \"\"\"\n",
    "    conversation.add_user_input(llm_input)\n",
    "    # 將 conversation.messages 丟給 chatbot\n",
    "    chatbot_result = chatbot(conversation.messages)[0]['generated_text']\n",
    "    print(f\"AI: {chatbot_result}\")\n",
    "    conversation.append_response(chatbot_result)\n",
    "    \n",
    "    user_query = input(\">>>\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
